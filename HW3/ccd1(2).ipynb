{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCD Basics I  Photometric data\n",
    "________________________________________________________________________________________-\n",
    "\n",
    "This exercise is adapted from an introductory tutorial to IRAF (Image Reduction and Analysis Facility): http://iraf.noao.edu/ \n",
    "\n",
    "The IRAF tools are now no longer supported by the National Optical Astronomy Observatory, but most have been imported into equivalent python routines within the Astropy package. Right now, however, we're just using basic image math to apply the necessary corrections to data frames.\n",
    "\n",
    "This exercise is designed to show you how to accomplish preliminary reductions of CCD data, including:\n",
    " - trimming of the overscan region\n",
    " - subtraction of the bias or zero level, and \n",
    " - flat field correction.\n",
    "\n",
    "The images for this exercise are direct imaging data taken at the Kitt Peak National Observatory by Dr. George Jacoby. The files are called m92001.fits, m92006.fits, etc.\n",
    "\n",
    "These image files are the data and calibration frames for some images of M92. There is one master bias or \"zero\" frame and also two flat field frames and four images all taken through either B or V filters.\n",
    "\n",
    "We will work through spectroscopic data reductions (files: sp\\*.fits) in another excercise.\n",
    "\n",
    "This exercise assumes that you have:\n",
    "(1) installed Astroconda on your laptop and \n",
    "(2) worked through the previous homework assignments and have some familiarity with the basics of python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports of python packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change some default plotting parameters\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['image.origin'] = 'lower'\n",
    "mpl.rcParams['image.interpolation'] = 'nearest'\n",
    "mpl.rcParams['image.cmap'] = 'Greys_r'\n",
    "\n",
    "# run the %matplotlib magic command to enable inline plotting\n",
    "# in the current Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling FITS files\n",
    "\n",
    "For more information about astropy.io.fits, see http://docs.astropy.org/en/stable/io/fits/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import astropy fits file handling package and read data array from fits file\n",
    "from astropy.io import fits\n",
    "sci_fn = 'm92001.fits'\n",
    "sci_hdulist = fits.open(sci_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give fits info\n",
    "sci_hdulist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data array and list size and data type\n",
    "data = sci_hdulist[0].data.astype(np.float)\n",
    "print(data.shape)\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FITS headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data header and print it\n",
    "hdr = sci_hdulist[0].header\n",
    "hdr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** This file is an average of several bias frames. Investigate the header information above to find out how many frames were averaged to create this file. \n",
    "\n",
    "For the data reductions for your observing project, you will create a master bias like this one from the bias frames you take at the 16-inch telescope. \n",
    "\n",
    "Let's explore the header information a bit more now. From the output above, we see that the type of data file is listed in the \"IMAGETYP\" keyword of the header.\n",
    "\n",
    "We can also get header information by keyword indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr['IMAGETYP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get the image header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io.fits import getheader\n",
    "hdr = getheader('m92001.fits')                              \n",
    "val = hdr[10]                       # get the 11th keyword's value *remember 0 indexing!\n",
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**QUESTION:** The value of the 10th keyword is ____? What is the name of 10th header keyword? (Compare with the full header above)\n",
    "\n",
    "**QUESTION:** Add a cell below to get the DATE of the observation.\n",
    "\n",
    "Astropy installs some useful scripts you can use on the command line (outside of this notebook) to inspect the headers of your FITS files: http://docs.astropy.org/en/stable/io/fits/usage/scripts.html\n",
    "\n",
    "In a terminal window running astroconda (i.e. after typing \"source activate astroconda3\") use the fitsheader tool to look at the IMAGETYP keyword of all the m92\\*.fits files:\n",
    " \n",
    " > fitsheader --keyword IMAGETYP m92\\*.fits\n",
    "\n",
    "**QUESTION:** Copy and paste the output of the fitsheader command above. Now we know the values of the IMAGETYP for each image in the M92 observation set in your data directory\n",
    "\n",
    "\n",
    "## Image display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the data file \n",
    "from astropy.visualization import (MinMaxInterval, SqrtStretch, ImageNormalize)\n",
    "norm = ImageNormalize(data, interval=MinMaxInterval(),stretch=SqrtStretch())\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "im = ax.imshow(data, origin='lower', norm=norm)\n",
    "fig.colorbar(im)\n",
    "#plt.imshow(data, scale='sqrt', percent=99.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot a histogram of data values\n",
    "plt.hist(data.ravel(), bins=20, range=(0,800))\n",
    "plt.xlabel(\"Pixel value\")\n",
    "plt.ylabel(\"No. pixels\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Comment on the appearance of the greyscale image of the data frame you made above. (Recall the image type!)\n",
    "\n",
    "**QUESTION:** How does the histogram relate to the appearance of the greyscale image?\n",
    "\n",
    "\n",
    "## Overscan trimming\n",
    "\n",
    "We need to find the location of the overscan region on the CCD chip, which determines the trimming parameters and the output image size. For this chip the overscan region is the last several columns. The overscan region and any bad rows or columns along the edges of the frame are then trimmed from the image to produce a trimmed output image (still unreduced). \n",
    "\n",
    "We determine these parameters by examining an image OTHER THAN the master bias frame in ds9. A flat field works best.\n",
    "*Note: Under the Scale menu, uncheck \"Use DATASEC\" if it is checked. Also, it may help here to adjust the scale to 99% or similar using the top menu bar.*\n",
    "\n",
    "Use the ds9 window to determine the overscan region, i.e. the columns where the count values drop to a level that is approximately the level of the master bias frame you've been looking at up to this point.\n",
    "\n",
    "**QUESTION:** What columns are these overscan columms?\n",
    "\n",
    "**QUESTION:** How does your answer to the previous question relate to the DATASEC and BIASSEC parameters in the header information above? What is the convention for specifying the columns & rows? \n",
    "\n",
    "**QUESTION:** In an xterm window running astroconda3, change directories to the data directory where the m92\\*.fits files are found. Verify that the DATASEC in the header above applies to all the frames by using the fitsheader command:\n",
    "\n",
    "> fitsheader --keyword DATASEC m92\\*.fits\n",
    "\n",
    "Copy & paste the output below. Now do the same to verify that all the BIASSEC values are the same in each file.\n",
    "\n",
    "\n",
    "\n",
    "Now, let's trim the overscan region from all our images. The syntax for trimming the data array (remember, this is still just our master bias image from above) to keep all of the first index but only the first N rows of the second index is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace N with the correct value, corresponding to (no. columns in the DATASEC above) -1 (zero indexing!)\n",
    "masterbias_trimmed = data[:,:N]\n",
    "print(masterbias_trimmed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the Python tutorial, specifically the Numpy section on indexing and slicing for more \n",
    "information: <br>\n",
    "http://www.scipy-lectures.org/intro/numpy/array_object.html#indexing-and-slicing\n",
    "\n",
    "Now, write this new trimmed image to a fits file. (This step isn't always necessary. If you're just going to move on to futher reduction steps that use this trimmed array, you might not need to write out this array to a fits file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu = fits.PrimaryHDU(masterbias_trimmed)\n",
    "hdu.writeto('trim_m92001.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subtraction of the overscan needs to be done to all the other FITS files in this directory by starting with the appropriate steps above to read the FITS data into an array and trim the array as in the previous step. \n",
    "\n",
    "Let's devise a quick way to do this. In this directory where your ccd1.ipynb file and all your data files are located, make a list called \"files.list\" of all the files *excluding* the master bias frame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=np.dtype([('filename', np.unicode, 20)])\n",
    "files=np.genfromtxt(\"files.list\",dtype=dt)\n",
    "infiles=files['filename']\n",
    "\n",
    "for i in range (0,len(infiles)):\n",
    "    sci_fn = infiles[i]\n",
    "    out= \"trim_\"+infiles[i]\n",
    "    sci_hdulist = fits.open(sci_fn)\n",
    "    data = sci_hdulist[0].data.astype(np.float)\n",
    "    trimmed=data[:,:320]\n",
    "    hdu = fits.PrimaryHDU(trimmed)\n",
    "    hdu.writeto(out)\n",
    "    print (\"writing \", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that you now have a set of FITS files called trim\\_\\*.fits corresponding to each file you started out with.\n",
    "\n",
    "## Bias subtraction\n",
    "\n",
    "The next step in the reduction process is to remove the bias level from all frames. The bias level is contained in the your trimmed master bias frame, here called trim\\_m92001.fits \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_fn = 'trim_m92001.fits'\n",
    "sci_hdulist = fits.open(sci_fn)\n",
    "bias = sci_hdulist[0].data.astype(np.float)\n",
    "\n",
    "for i in range (0,len(infiles)):\n",
    "    sci_fn = \"trim_\"+infiles[i]\n",
    "    out= \"bias_\"+infiles[i]\n",
    "    sci_hdulist = fits.open(sci_fn)\n",
    "    data = sci_hdulist[0].data.astype(np.float)\n",
    "    biassub=data-bias\n",
    "    hdu = fits.PrimaryHDU(biassub)\n",
    "    hdu.writeto(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that you now have a set of files called bias\\_m92006.fits, bias\\_m92007.fits, etc. These are the frames that have been both trimmed and bias subtracted.\n",
    "\n",
    "**QUESTION:** Calculate the image statistics (mean, median, standard deviation, min & max) on: (1) the master bias frame; (2) the pre-bias-corrected frames; and (3) the post-bias-corrected frames and note them here. Are these values consistent with your expectations?\n",
    "\n",
    "\n",
    "## Flat field\n",
    "\n",
    "We have finally arrived at the flat fielding stage. We have two flats and they need to be normalized before we divide our object frames by *the flat field frame appropriate for its filter*.\n",
    "\n",
    "We will use image statistics to determine the normalization value for each flat, and then use our same image math techniques to create the normalized flats.\n",
    "\n",
    "You now have all the tools to perform these tasks. Here are the steps:\n",
    "\n",
    " 1. Determine which frame is the flat in the B filter and which is the flat in the V filter by examinine the header information. Usually that is in a FILTER header keyword. Here the information you need is in the OBJECT keyword:\n",
    "\n",
    " > fitsheader --keyword OBJECT m92\\*.fits\n",
    "\n",
    " 2. Find the mode of the B and V flats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NN with the correct number for the B flat field image\n",
    "from scipy import stats\n",
    "\n",
    "sci_fn = \"bias_m92014.fits\"\n",
    "sci_hdulist = fits.open(sci_fn)\n",
    "Bflat = sci_hdulist[0].data.astype(np.float)\n",
    "stats.mode(Bflat, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3.Normalize the flat field by its mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace MODE with the value of the previous operation.\n",
    "nBflat=Bflat/MODE\n",
    "nBflat[nBflat == 0] = 1.  \n",
    "#This last step replaces any pixels that are =0 with a value of 1 in the normalized flat. This\n",
    "#is necessary to avoid errors when we divide by this frame below.\n",
    "\n",
    "#Add extra cells to repeat the previous step and this step for the flat in the V filter.\n",
    "#Call the files Vflat and nVflat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What effect does this division by the mode have on the image, i.e. why do we refer to this as normalizing the flat field frames?\n",
    "\n",
    "**QUESTION:** What are the statistics (mean, median, standard deviation) for these normalized flat fields? Comment on the values of the mean & median for a normalized image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Now divide each of the object frames by the appropriate normalized flat for that filter. And write to a new fits file. Add extra cells for the additional data frames as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NN as two cells above\n",
    "sci_fn = \"bias_m920NN.fits\"\n",
    "sci_hdulist = fits.open(sci_fn)\n",
    "data = sci_hdulist[0].data.astype(np.float)\n",
    "fdata= data/nBflat #or nVflat\n",
    "hdu = fits.PrimaryHDU(fdata)\n",
    "hdu.writeto('flat_m920NN.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at these final images with ds9. Check to see if the background is flat across the image.  Sometimes the dome flats are not sufficient for flattening images and additional sky flats may need to be used. For your observations, you'll just use these sky flats, which can be normalized in the same way.\n",
    "\n",
    "You now have a set of images that have been zero and flat corrected. As noted above, you will not have to do an overscan substraction for the data frames for your observing project. You will however, need to do a dark current subtraction after averaging your dark current frames and scaling them by the exposure time in your data frames.\n",
    "\n",
    "Write the commands for flat field correction into your ccd1\\_reduce.py script for later use. You will not necessarily need to use a for loop for these, due to the small number of frames involved in each calculation, but depending on the number of files you have to reduce for your project, you may want to edit this later. \n",
    "\n",
    "Often in data reduction and analysis, there are many choices of methods that all give the correct output, just choose what makes the most sense for you. It is good practice to devise ways to check your results by calculating image statistics or using a viewer like ds9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
