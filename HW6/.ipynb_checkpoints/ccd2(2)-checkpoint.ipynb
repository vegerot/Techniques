{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCD Basics II Spectroscopy: Reducing spectral data\n",
    "________________________________________________________________________________________-\n",
    "\n",
    "This exercise is adapted from an introductory tutorial to IRAF (Image Reduction and Analysis Facility): http://iraf.noao.edu/ \n",
    "\n",
    "The IRAF tools are now no longer supported by the National Optical Astronomy Observatory, but most have been imported into equivalent python routines within the Astropy package. Right now, however, we're just using basic image math to apply the necessary corrections to data frames.\n",
    "\n",
    "This exercise is designed to show you how to accomplish preliminary reductions of spectroscopic CCD data, including: \n",
    " - trimming overscan regions\n",
    " - creation of a master bias\n",
    " - subtraction of the bias level\n",
    " - creation and normalization of a response function using a master (dome) flat\n",
    " - creation and normalization of an illumination correction using a master (sky) flat\n",
    " - use both the response & illumination to flat field the spectral data on the CCD chip.\n",
    "\n",
    "The spectra for this exercise were taken at the Coude Feed telescope at Kitt Peak National Observatory by Dr. Phil Massey. The files are called sp\\*.fits, and are found in on vega2 in /data/ASTR303/HW4).\n",
    "\n",
    "## Examining the data\n",
    "\n",
    "Load one of the object frames into ds9 to inspect it. You should see a bright vertical stripe not quite centered on a dark bacgkround. (If it's centered, go into the Scale menu and uncheck \"Use DATASEC\".)\n",
    "\n",
    "The vertical stripe is the spectrum. Here, the dispersion direction is vertical, i.e. along the columns, and the spatial direction is horizontal. So, when we trace and extract the spectrum and ultimately do our wavelength calibration, we'll do all of that along columns of the CCD. \n",
    "\n",
    "*For any 16\" spectral data you take, this will be rotated 90 degrees, with the dispersion along the rows and the spatial direction along the columns.*\n",
    "\n",
    "Now load one of your sky flats.\n",
    "\n",
    "**QUESTION:** How does this differ from the object spectrum? Why? (Think about how the slit is illuminated for a sky flat versus for a spectrum of a star.)\n",
    "\n",
    "Now load one of your dome flats.\n",
    "\n",
    "**QUESTION:** How does this differ from the sky flat spectrum? Why? \n",
    "\n",
    "Now on your computer in the directory you're working, **make 4 lists of the sp\\*.fits files- one list for the bias frames, one for the flat fields taken on the twilight sky (sky flats), one for the regular (dome) flats, and one for the data frames.** There are also files called \"comp\" files, for comparison spectra. We'll use those in the next exercise to do the dispersion solution, describing the wavelength per pixel on the CCD.\n",
    "\n",
    "You can figure out which file is which type using the \"fitsheader\" command (see ccd1.ipynb) or by loading them into ds9.\n",
    "\n",
    "\n",
    "## Trim overscan\n",
    "\n",
    "Remember from the previous ccd1 exercise that the overscan region (if one exists on your chip- it does not for the 16\" telescope data) is best found using a flat field frame with a large number of counts. Use the dome flat field you should have in ds9 from the previous question to find the columns corresponding to the overscan, i.e. the columns at the right of the image where the count level drops from the level in the *unexposed* region of the chip.\n",
    "\n",
    "**QUESTION: ** Enter your overscan columns in the usual [colmin:colmax,rowmin:rowmax] format.\n",
    "\n",
    "**QUESTION: ** Does this agree with the BIASSEC keyword in the header? \n",
    "\n",
    "**QUESTION: ** What is the DATASEC keyword?  You will use this below to trim your data array once we read in the FITS data.\n",
    "\n",
    "*Overscan regions for each CCD frame are used  in contexts in which the bias level changes rapidly. Rather than combining many zero frames together, one could use the bias level measured in columns of the overscan region to apply a correction across all the columns on the CCD chip. Here, we're just going to combine several bias/zero frames together to make a master bias and use that correct for the bias level in all the other frames (flats and data frames), as you'll do for your observing project.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change some default plotting parameters\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['image.origin'] = 'lower'\n",
    "mpl.rcParams['image.interpolation'] = 'nearest'\n",
    "mpl.rcParams['image.cmap'] = 'Greys_r'\n",
    "\n",
    "# run the %matplotlib magic command to enable inline plotting\n",
    "# in the current Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#import astropy fits file handling package\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading FITS files and extracting data\n",
    "\n",
    "astropy.io.fits  http://docs.astropy.org/en/stable/io/fits/index.html\n",
    "\n",
    "Now, we will do one example of reading in FITS data and subtracting this overscan region, and then we will apply that method to all the FITS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data array from fits file\n",
    "sci_fn = 'sp0020.fits'  \n",
    "sci_hdulist = fits.open(sci_fn)\n",
    "data = sci_hdulist[0].data.astype(np.float) # define the data array and list size and data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace N with the correct value, corresponding to no. columns in the DATASEC above\n",
    "data_trimmed = data[:,:100] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Of the data types in the lists you made, which one applies to the sp0020.fits image?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the data_trimmed array\n",
    "plt.imshow(data_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()  # Start a new plot -- by default matplotlib overplots.\n",
    "plt.plot(data_trimmed[300,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = data_trimmed.mean(axis=1)\n",
    "plt.figure()\n",
    "plt.plot(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp_all.list is my ASCII file containing all the FITS data files- bias, flats, and object frames\n",
    "dt=np.dtype([('filename', np.unicode, 20)])\n",
    "files=np.genfromtxt(\"sp_all.list\",dtype=dt)\n",
    "infiles=files['filename']\n",
    "\n",
    "#loop over all FITS files, read them in, then read out all but the overscan region\n",
    "#write the output to a file prepended with \"trim\\_\"\n",
    "for i in range (0,len(infiles)):\n",
    "    sci_fn = infiles[i]\n",
    "    out= \"trim_\"+infiles[i]\n",
    "    sci_hdulist = fits.open(sci_fn)\n",
    "    data = sci_hdulist[0].data.astype(np.float)\n",
    "    trimmed=data[:,:100]\n",
    "    hdu = fits.PrimaryHDU(trimmed)\n",
    "    hdu.writeto(out)\n",
    "for file in infiles\n",
    "    out='trim'+file\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that you now have a set of files called trim\\_sp001.fits, trim\\_sp002.fits, etc.\n",
    "\n",
    "\n",
    "## Bias correction\n",
    "Next, we need to create a MasterBias frame from all the trimmed bias frames. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nccd=np.shape(trimmed)\n",
    "nrow=nccd[0]  #here, 1024\n",
    "ncol=nccd[1]  #here, 100\n",
    "\n",
    "#read in all the trimmed zero frames to make the master bias\n",
    "#sp_bias.list is my list of all the bias frames\n",
    "dt=np.dtype([('filename', np.unicode, 20)])\n",
    "files=np.genfromtxt(\"sp_bias.list\",dtype=dt)\n",
    "infiles=files['filename']\n",
    "\n",
    "#Code uses output of previous loop- i.e. frames with overscan trimmed are prepended with \"trim_\".\n",
    "alldata=np.ones((len(infiles),nrow,ncol))\n",
    "for i in range (0,len(infiles)):\n",
    "    sci_fn = \"trim_\"+infiles[i]\n",
    "    sci_hdulist = fits.open(sci_fn)\n",
    "    alldata[i,:,:] = sci_hdulist[0].data.astype(np.float)\n",
    "\n",
    "masterbias=np.ones((nrow,ncol))\n",
    "masterbias= np.median(alldata,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have not written the masterbias array to a fits file. There's no need to do that at the moment, as we'll just use the array within this notebook later.\n",
    "\n",
    "**QUESTION** What statistical function are we using to combine the frames? Why is this function advantageous? Recall that this is done pixel-by-pixel.\n",
    "\n",
    "Now, we need to subtract this MasterBias from all our remaining frames, i.e. the sky flats, dome flats, and object frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct all the sky flat, flat, object frames using the master bias\n",
    "\n",
    "#sky flats first- sp_skyflat.list is my list of sky flats\n",
    "dt=np.dtype([('filename', np.unicode, 20)])\n",
    "files=np.genfromtxt(\"sp_skyflat.list\",dtype=dt)\n",
    "infiles=files['filename']\n",
    "\n",
    "for i in range (0,len(infiles)):\n",
    "    sci_fn = \"trim_\"+infiles[i]\n",
    "    out= \"biascor_\"+infiles[i]\n",
    "    sci_hdulist = fits.open(sci_fn)\n",
    "    trimmed = sci_hdulist[0].data.astype(np.float)\n",
    "    biascor= trimmed-masterbias\n",
    "    hdu = fits.PrimaryHDU(biascor)\n",
    "    hdu.writeto(out)\n",
    "\n",
    "#dome flats- sp_flat.list is my list of dome flats\n",
    "files=np.genfromtxt(\"sp_flat.list\",dtype=None,names=('filename'))\n",
    "infiles=files['filename']\n",
    "\n",
    "for i in range (0,len(infiles)):\n",
    "    sci_fn = \"trim_\"+infiles[i]\n",
    "    out= \"biascor_\"+infiles[i]\n",
    "    sci_hdulist = fits.open(sci_fn)\n",
    "    trimmed = sci_hdulist[0].data.astype(np.float)\n",
    "    biascor= trimmed-masterbias\n",
    "    hdu = fits.PrimaryHDU(biascor)\n",
    "    hdu.writeto(out)\n",
    "\n",
    "#object frames--- you write this loop, by adapting the loops for the other frame types above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Field\n",
    "\n",
    "As you did for the MasterBias, combine the sky flats into a MasterSky frame and your dome flats into a MasterFlat frame using the median function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the sky flats, make master Sky\n",
    "dt=np.dtype([('filename', np.unicode, 20)])\n",
    "files=np.genfromtxt(\"sp_skyflat.list\",dtype=dt)\n",
    "infiles=files['filename']\n",
    "\n",
    "alldata=np.ones((len(infiles),nrow,ncol))\n",
    "for i in range (0,len(infiles)):\n",
    "    sci_fn = \"biascor_\"+infiles[i]\n",
    "    sci_hdulist = fits.open(sci_fn)\n",
    "    alldata[i,:,:] = sci_hdulist[0].data.astype(np.float)\n",
    "mastersky=np.ones((nrow,ncol))\n",
    "mastersky= np.median(alldata,axis=0)\n",
    "hdu = fits.PrimaryHDU(mastersky)\n",
    "hdu.writeto(\"MasterSky.fits\")\n",
    "\n",
    "#read in the dome flats, make master Flat\n",
    "dt=np.dtype([('filename', np.unicode, 20)])\n",
    "files=np.genfromtxt(\"sp_flat.list\",dtype=dt)\n",
    "infiles=files['filename']\n",
    "\n",
    "alldata=np.ones((len(infiles),nrow,ncol))\n",
    "for i in range (0,len(infiles)):\n",
    "    sci_fn = \"biascor_\"+infiles[i]\n",
    "    sci_hdulist = fits.open(sci_fn)\n",
    "    alldata[i,:,:] = sci_hdulist[0].data.astype(np.float)\n",
    "masterflat=np.ones((nrow,ncol))\n",
    "masterflat= np.median(alldata,axis=0)\n",
    "hdu = fits.PrimaryHDU(masterflat)\n",
    "hdu.writeto(\"MasterFlat.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've created our MasterSky and MasterFlat frames we can flat field the spectral data. This consists of two two corrections- one for Reponse (wavelength direction, here: along columns) and one for Illumination (spatial direction, here: along rows)\n",
    "\n",
    "### Response\n",
    "\n",
    "This portion of the flat field takes out variations in the dispersion direction (for these frames, along the columns). For this we have to be careful not to remove any true variation with wavelength so we will look at the shape of our Master Flat file along the dispersion direction (i.e. as a function of row number) then fit a function to this. \n",
    "We'll divide each row on in our MasterFlat by that function to create our Response image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import astropy fits file handling package and read data array from fits file\n",
    "from astropy.io import fits\n",
    "sci_fn = 'MasterFlat.fits'  #this is my median combination of the dome flats\n",
    "sci_hdulist = fits.open(sci_fn)\n",
    "masterflat = sci_hdulist[0].data.astype(np.float) # define the data array and list size and data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(masterflat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the shape of our trimmed array, 1024 rows by 100 columns. \n",
    "Let's look at the slit profile, by taking a slice along row 512 of the chip: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()  \n",
    "plt.plot(masterflat[512,:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that column 50 or so falls right in the middle of the spatial profile, so to see if there is any difference in the CCD response along the wavelength direction, we'll plot a slice down column 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()  # Start a new plot -- by default matplotlib overplots.\n",
    "plt.plot(masterflat[:,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite a dramatic variation of the counts in the wavelength direction, so let's fit a function to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x=np.arange(0, 1024)\n",
    "y=masterflat[:,50]\n",
    "pfit = np.polyfit(x, y, 2)  # Fit a 2nd order polynomial to (x, y) data\n",
    "yfit = np.polyval(pfit, x)  # Evaluate the polynomial at x\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,yfit)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll take that function and divide the columns in our Master Flat by that to create the Response array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfit=1./yfit\n",
    "yfit=yfit[:,np.newaxis]\n",
    "response=np.multiply(yfit,masterflat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()  # Start a new plot -- by default matplotlib overplots.\n",
    "plt.plot(response[:,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()  # Start a new plot -- by default matplotlib overplots.\n",
    "plt.plot(response[512,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Compare the y-axis values on this plot to the ones in the slit profile above. Is this reponse normalized? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the response function to a fits file in case you need it later\n",
    "hdu = fits.PrimaryHDU(response)\n",
    "hdu.writeto(\"Response.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illumination\n",
    "\n",
    "For this, we essentially do the same thing as in the case of the Reponse but we are generating a fit along the spatial profile that we apply across all rows. For this, we'll use the Master Sky flat we've created.\n",
    "\n",
    "Let's load the MasterSky frame you made from the median of your sky frames, then divide the MasterSky by the Response array we derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_fn = 'MasterSky.fits'  #this is my median combination of the sky flats\n",
    "sci_hdulist = fits.open(sci_fn)\n",
    "mastersky = sci_hdulist[0].data.astype(np.float) # define the data array and list size and data type\n",
    "mastersky_resp=mastersky/response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MasterSky or MasterFlat frame into ds9.\n",
    "\n",
    "**QUESTION:** In what column does the illumination begin? In what column does it end?\n",
    "\n",
    "We need to divide out the differences in illumination of the sky across the slit. So these are the columns we'll use to make our fit, binning up along the dispersion direction, say by 32. So we'll start with rows 1-32 (or 0-31 in python) and apply one fit to all those rows, then a second fit to rows 32-64, etc.\n",
    "\n",
    "We'll go through it once, and then a few cells below you'll see another cell with a \"for loop\" that repeats the process the (1024/32)X needed to reach all the way across the spectrum on the CCD. Later, we'll subtract the sky spectrum (an average from extractions above & below the slit) from the object spectrum.\n",
    "\n",
    "Let's look at the spatial profile- i.e. the way the slit is illuminated in the spatial direction (across the rows). We'll choose the row in the center of the chip, 512.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()  # Start a new plot -- by default matplotlib overplots.\n",
    "plt.plot(mastersky[512,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this and/or from ds9 it looks like columns 33-75 are reasonable to use to fit a function to the slit illumination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(33,75)  # Background columns\n",
    "sky=mastersky_resp[0:31,33:75]\n",
    "y=np.median(sky,axis=0) # median of background in rows 1-32 over columns 33-75 of normalized master sky\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "#Replace N with the order of the polynomial you want to try first, you can alwasy come back and change that.\n",
    "pfit = np.polyfit(x, y, 2)  # Fit a N order polynomial to (x, y) data\n",
    "yfit = np.polyval(pfit, x)  # Evaluate the polynomial at x\n",
    "plt.plot(x, yfit)\n",
    "plt.grid()\n",
    "np.sum( (yfit/5 - y/5)**2/(y/5.) )  #This is a rough estimate of the chi-squared (goodness-of-fit) parameter for these data\n",
    "#Does this give a good fit?  Try adjusting the order of the polynomial until the residuals are less than one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that we want to divide the first 32 rows of the data by,repeating the fit, normalization, and division for each 32 row increment. Let's add this as a do loop to your ccd2\\_reduce.py script.\n",
    "(There's no need to make plots for each one, just use the same order fit you found above for all.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#illumination correction\n",
    "sky_fn=\"MasterSky.fits\"\n",
    "sky_hdulist = fits.open(sky_fn)\n",
    "mastersky = sky_hdulist[0].data.astype(np.float)\n",
    "\n",
    "resp_fn=\"Response.fits\"\n",
    "resp_hdulist= fits.open(resp_fn)\n",
    "response= resp_hdulist[0].data.astype(np.float)\n",
    "\n",
    "#remove the response in the dispersion direction we found earlier\n",
    "#we'll multiply it back in later to create the final \"superFlat\" frame\n",
    "mastersky_resp=mastersky/response\n",
    "x = np.arange(33,75)  # Background columns\n",
    "nr=1024/32\n",
    "i1=0\n",
    "for i in range (0,32):\n",
    "    i2=i1+nr-1\n",
    "    sky=mastersky_resp[i1:i2,33:75]\n",
    "    y=np.median(sky,axis=0) # median of background in 50 rows at a time over columns 33-75 of normalized master sky\n",
    "    pfit = np.polyfit(x, y, 3)  # Fit a N order polynomial to (x, y) data\n",
    "    yfit = np.polyval(pfit, x)  # Evaluate the polynomial at x\n",
    "    mastersky_norm=mastersky_resp[:,33:75]\n",
    "    i3=i2+1\n",
    "    for ii in range (i1,i3):\n",
    "#        mastersky_norm[ii]=mastersky_norm[ii]/yfit\n",
    "        mastersky_norm[ii]=yfit/np.median(yfit)\n",
    "    i1=i2+1\n",
    "\n",
    "hdu = fits.PrimaryHDU(mastersky_norm)\n",
    "hdu.writeto(\"MasterSkyNorm.fits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to broadcast this array (1024 x 44) into an array the size of the trimmed array 1024 x 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcols= np.ones((1024, 29))\n",
    "illumination=np.hstack((newcols,mastersky_norm,newcols))\n",
    "hdu = fits.PrimaryHDU(illumination)\n",
    "hdu.writeto(\"Illumination.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcols= np.ones((1024, 29))\n",
    "illumination=np.hstack((newcols,mastersky_norm,newcols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now  multiply by the Response frame to make a SuperFlat, a flat field containing the corrections in both the dispersion (from Response) and spatial directions (from Illumination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superflat= illumination * response\n",
    "hdu = fits.PrimaryHDU(superflat)\n",
    "hdu.writeto(\"SuperFlat.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, divide your object spectra by this SuperFlat frame and you're ready to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide data frames by the SuperFlat\n",
    "dt=np.dtype([('filename', np.unicode, 20)])\n",
    "files=np.genfromtxt(\"sp_obj.list\",dtype=dt)\n",
    "infiles=files['filename']\n",
    "\n",
    "for i in range (0,len(infiles)):\n",
    "    sci_fn = \"biascor_\"+infiles[i]\n",
    "    out= \"flat_\"+infiles[i]\n",
    "    sci_hdulist = fits.open(sci_fn)\n",
    "    biascor = sci_hdulist[0].data.astype(np.float)\n",
    "    flatfielded = biascor/superflat\n",
    "    hdu = fits.PrimaryHDU(flatfielded)\n",
    "    hdu.writeto(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Add a few cells here at the end to check the statistics (mean, median, standard deviation) of your final flat field, i.e. the superflat. \n",
    "\n",
    "**QUESTION**\n",
    "What values do you expect to see? Is this what you find?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
